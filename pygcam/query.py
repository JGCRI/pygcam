"""
.. Support for querying GCAM's XML database and processing results.

.. codeauthor:: Rich Plevin <rich@plevin.com>

.. Copyright (c) 2016 Richard Plevin
   See the https://opensource.org/licenses/MIT for license details.

"""
import os
import re
import subprocess

from .log import getLogger
from .common import getTempFile, mkdirs, ensureExtension
from .Xvfb import Xvfb
from .config import getParam, getParamAsBoolean, readConfigFiles
from .error import PygcamException, ConfigFileError, FileFormatError

_logger = getLogger(__name__)

GCAM_32_REGIONS = [
    'Africa_Eastern',
    'Africa_Northern',
    'Africa_Southern',
    'Africa_Western',
    'Argentina',
    'Australia_NZ',
    'Brazil',
    'Canada',
    'Central America and Caribbean',
    'Central Asia',
    'China',
    'Colombia',
    'EU-12',
    'EU-15',
    'Europe_Eastern',
    'Europe_Non_EU',
    'European Free Trade Association',
    'India',
    'Indonesia',
    'Japan',
    'Mexico',
    'Middle East',
    'Pakistan',
    'Russia',
    'South Africa',
    'South America_Northern',
    'South America_Southern',
    'South Asia',
    'South Korea',
    'Southeast Asia',
    'Taiwan',
    'USA'
]

def limitYears(df, years):
    """
    Modify df to drop all years outside the range given by `years`.

    :param years: a sequence of two years (str or int); only values in this
        range (inclusive) are kept. Data for other years is dropped.
    :return: (DataFrame) df, modified in place.
    """
    first, last = map(int, years)
    yearCols  = map(int, filter(str.isdigit, df.columns))
    dropYears = map(str, filter(lambda y: y < first or y > last, yearCols))
    df.drop(dropYears, axis=1, inplace=True)
    return df

def interpolateYears(df, startYear=0, inplace=False):
    """
    Interpolate linearly between each pair of years in the GCAM output. The
    timestep is calculated from the numerical (string) column headings given
    in the `DataFrame`_ `df`, which are assumed to represent years in the time-series.
    The years to interpolate between are read from `df`, so there's no dependency
    on any particular time-step, or even on the time-step being constant.

    :param df: (DataFrame) Data of the format returned by batch queries
        on the GCAM XML database
    :param startYear: If non-zero, begin interpolation at this year.
    :param inplace: If True, modify `df` in place; otherwise modify a copy.
    :return: if `inplace` is True, `df` is returned; otherwise a copy
      of `df` with interpolated values is returned.
    """
    yearCols = filter(str.isdigit, df.columns)
    years = map(int, yearCols)

    for i in range(0, len(years)-1):
        start = years[i]
        end   = years[i+1]
        timestep = end - start

        if timestep == 1:       # don't interpolate annual results if already annual
            continue

        startCol = df[str(start)]
        endCol   = df[str(end)]

        # compute vector of annual deltas for each row
        delta = (endCol - startCol)/timestep

        # interpolate the whole column -- but don't interpolate before the start year
        for j in range(1, timestep):
            nextYear = start + j
            df[str(nextYear)] = df[str(nextYear-1)] + (0 if nextYear < startYear else delta)

    yearCols = filter(str.isdigit, df.columns)  # get annualized year columns
    years = map(int, yearCols)       # sort as integers
    years.sort()
    yearCols = map(str, years)       # convert back to strings, now sorted

    nonYearCols = list(set(df.columns) - set(yearCols))
    result = df.reindex_axis(nonYearCols + yearCols, axis=1, copy=(not inplace))
    return result

def readCsv(filename, skiprows=1, years=None, interpolate=False, startYear=0):
    """
    Read a CSV file of the form generated by GCAM batch queries, i.e., skip on
    row and then read column headings and data. Optionally drop all years outside
    the `years` given. Optionally linearly interpolate annual values between
    timesteps.

    :param filename: (str) the path to a CSV file
    :param skiprows: (int) the number of rows to skip before reading the data matrix
    :param years: (iterable of two values coercible to int) the year columns to keep; others are dropped
    :param interpolate: (bool) If True, interpolate annual values between timesteps
    :param startYear: (int) If interpolating, the year to begin interpolation
    :return: (DataFrame) the data read in, optionally processed as per arguments
    """
    import pandas as pd

    _logger.debug("    Reading", filename)
    try:
        df = pd.read_table(filename, sep=',', skiprows=skiprows, index_col=None)
    except IOError, e:
        raise PygcamException("*** Reading file '%s' failed: %s\n" % (filename, e))

    if years:
        limitYears(df, years)

    if interpolate:
        df = interpolateYears(df, startYear=startYear)

    return df

def readQueryResult(batchDir, baseline, queryName, years=None, interpolate=False, startYear=0):
    """
    Compose the name of the 'standard' result file, read it into a DataFrame and
    return the DataFrame. Data is read from the computed filename
    "{batchDir}/{queryName}-{baseline}.csv".

    :param batchDir: (str) a directory in which the data file resides
    :param baseline: (str) the name of a baseline scenario
    :param queryName: (str) the name of a batch query.
    :param years: (iterable of two values coercible to int) the year columnss to keep; others are dropped
    :param interpolate: (bool) If True, interpolate annual values between timesteps
    :param startYear: (int) If interpolating, the year to begin interpolation
    :return: (DataFrame) the data in the computed filename.
    """
    pathname = os.path.join(batchDir, '%s-%s.csv' % (queryName, baseline))
    df= readCsv(pathname, years=years, interpolate=interpolate, startYear=startYear)
    return df

def getRegionList(workspace=None):
    """
    Get a list of the defined region names.

    :param workspace: the path to a ``Main_User_Workspace`` directory that
      has the file
      ``input/gcam-data-system/_common/mappings/GCAM_region_names.csv``,
      or ``None``, in which case the value of config variable
      ``GCAM.SourceWorkspace`` (if defined) is used. If `workspace` is
      empty or ``None``, and the config variable ``GCAM.SourceWorkspace`` is
      empty (the default value), the built-in default 32-region list is returned.
    :return: a list of strings with the names of the defined regions
    """
    relpath = 'input/gcam-data-system/_common/mappings/GCAM_region_names.csv'

    workspace = workspace or getParam('GCAM.SourceWorkspace')
    if not workspace:
        return GCAM_32_REGIONS

    path = os.path.join(workspace, relpath)

    _logger.debug("Reading:", path)
    df = readCsv(path, skiprows=3)  # this is a gcam-data-system input file (different format)
    regions = list(df.region)
    return regions

def readRegionMap(filename):
    """
    Read a region map file containing one or more tab-delimited lines of the form
    ``key`` <tab> ``value``, where `key` should be a standard GCAM region and
    `value` the name of the region to map the original to, which can be an
    existing GCAM region or a new name defined by the user.

    :param filename: the name of a file containing region mappings
    :return: a dictionary holding the mappings read from `filename`
    """
    import re
    mapping = {}
    pattern = re.compile('\t+')

    _logger.info("Reading region map '%s'", filename)
    with open(filename) as f:
        lines = f.readlines()

    for line in lines:
        line = line.strip()
        if line[0] == '#':
            continue

        tokens = pattern.split(line)
        if len(tokens) != 2:
            raise FileFormatError("Badly formatted line in region map '%s': %s" % (filename, line))

        mapping[tokens[0]] = tokens[1]

    return mapping

def dropExtraCols(df, inplace=True):
    """
    Drop some columns that GCAM queries sometimes return, but which we generally don't need.
    The columns to drop are taken from from the configuration file variable ``GCAM.ColumnsToDrop``,
    which should be a comma-delimited string. The default value is ``scenario,Notes,Date``.

    :param df: a `DataFrame`_ hold the results of a GCAM query.
    :param inplace: if True, modify `df` in-place; otherwise return a modified copy.
    :return: the original `df` (if inplace=True) or the modified copy.
    """
    columns = df.columns
    unnamed = 'Unnamed:'    # extra (empty) columns can sneak in; eliminate them
    dropCols = filter(lambda s: s[0:len(unnamed)] == unnamed, columns)

    varName = 'GCAM.ColumnsToDrop'
    colString = getParam(varName)
    colList = colString and colString.split(',')

    if colString and not colList:
        raise ConfigFileError("The value of %s is '%s'; should be a comma-delimited list of column names")

    unneeded  = set(colList)
    columnSet = set(columns)
    dropCols += columnSet & unneeded    # drop any columns in both sets

    resultDF = df.drop(dropCols, axis=1, inplace=inplace)
    return resultDF

def computeDifference(df1, df2):
    """
    Compute the difference between two `DataFrames`_.

    :param df1: a `DataFrame`_ instance
    :param obj2: a `DataFrame`_ instance
    :return: a `DataFrame`_ with the difference in all the year columns, computed
      as (df2 - df1).
    """
    df1 = df1.dropExtraCols(inplace=False)
    df2 = df2.dropExtraCols(inplace=False)

    if set(df1.columns) != set(df2.columns):
        raise PygcamException("Can't compute difference because result sets have different columns. df1:%s, df2:%s" \
                              % (df1.columns, df2.columns))

    yearCols = filter(str.isdigit, df1.columns)
    nonYearCols = list(set(df1.columns) - set(yearCols))

    df1.set_index(nonYearCols, inplace=True)
    df2.set_index(nonYearCols, inplace=True)

    # Compute difference for timeseries values
    diff = df2 - df1
    return diff

# def batchQueryToFile(scenario, database, queryName, outfile, queryPath=None):
#     """
#     Run a query against GCAM's XML database, and save the results
#     in `outfile`.
#
#     :param scenario: (str) the name of the scenario to perform the query on
#     :param database: (str) the pathname to the XML database to query
#     :param queryName: (str) the name of a query to execute
#     :param queryPath: (str) a colon-delimited string with paths on which to
#        search for the query either in a stand-alone XML file named {query}.xml,
#        or in any XML file named in the path, which should be formatted like
#        the standard GCAM file Main_Queries.xml.
#     :param outfile: (str) if None, query results are written to a temp file and read
#       into a `DataFrame`; if not None, the results are saved in `outfile`.
#     :return: a `DataFrame` holding the query results
#     """
#     jarFile     = getParam('GCAM.JarFile')
#     javaLibPath = getParam('GCAM.JavaLibPath')
#     javaArgs    = getParam('GCAM.JavaArgs')
#
#     basename = os.path.basename(queryName)
#     mainPart, extension = os.path.splitext(basename)   # strip extension, if any
#
#     # Look for both the literal name as given as well as the name with underscores replaced with spaces
#     filename, isTempFile = findOrCreateQueryFile(basename, queryPath, regions, regionMap=regionMap)
#
#     if not filename:
#         print "Error: file for query '%s' was not found." % basename
#         sys.exit(1)
#
#     tmpfile = False
#     if not outfile:
#         tmpfile = True
#         outfile = getTempFile('.csv')
#     try:
#         # run the query on database for scenario; results written to outfile
#         # TBD: implement this
#
#         df = readCsv(outfile, years=None, interpolate=False, startYear=0)
#         # obj = GcamResult(outfile)
#
#         if tmpfile:
#             os.unlink(outfile)      # remove the result file; not removed if error raised
#
#     except Exception as e:          # TBD: might want to trap unlink failure separately
#         raise
#
#     return df
#     # return obj if asDataFrame else obj.df

def _findOrCreateQueryFile(title, queryPath, regions, regionMap=None):
    '''
    Find a query with the given title either as a file (with .xml extension) or
    within an XML query file by searching queryPath. If the query with "title" is
    found in an XML query file, extract it to generate a batch query file and
    apply it to the given regions.
    '''
    items = queryPath.split(';')
    for item in items:
        if os.path.isdir(item):
            pathname = os.path.join(item, title + '.xml')
            if os.path.isfile(pathname):
                return (pathname, False)
        else:
            from lxml import etree as ET    # lazy import speeds startup

            tree = ET.parse(item)
            xpath = '/queries/queryGroup/*[@title="%s"]' % title
            elts = tree.xpath(xpath)  # returns empty list or list of elements found

            if elts is None or len(elts) == 0:
                # if the literal search fails, repeat search with all "-" or "_" changed to " ".
                xpath = '/queries/queryGroup/*[@title="%s"]' % re.sub('_', ' ', title)
                elts = tree.xpath(xpath)

            if elts is None or len(elts) == 0:
                # if the literal search fails, repeat search with all "-" or "_" changed to " ".
                xpath = '/queries/queryGroup/*[@title="%s"]' % re.sub('-', ' ', title)
                elts = tree.xpath(xpath)

            if elts is None or len(elts) == 0:
                # if the literal search fails, repeat search with all "-" or "_" changed to " ".
                xpath = '/queries/queryGroup/*[@title="%s"]' % re.sub('[-_]', ' ', title)
                elts = tree.xpath(xpath)

            if len(elts) == 1:
                elt = elts[0]
                root = ET.Element("queries")
                aQuery = ET.Element("aQuery")
                root.append(aQuery)
                for region in regions:
                    aQuery.append(ET.Element('region', name=region))

                aQuery.append(elt)

                if regionMap:
                    # if a rewrite list exists already, use it, otherwise create it.
                    subtree = ET.ElementTree(element=elt)
                    found = subtree.xpath('//labelRewriteList')
                    if len(found) == 1:
                        rewriteList = found[0]
                    else:
                        # create and inject the element
                        rewriteList = ET.Element('labelRewriteList')
                        elt.append(rewriteList)

                    level = ET.Element('level', name='region')
                    rewriteList.append(level)
                    for fromReg, toReg in regionMap.iteritems():
                        level.append(ET.Element('rewrite', attrib={'from': fromReg, 'to': toReg}))

                tmpFile = getTempFile('.xml')
                _logger.debug("Writing extracted query for '%s' to tmp file '%s'", title, tmpFile)
                tree = ET.ElementTree(root)
                tree.write(tmpFile, xml_declaration=True, encoding="UTF-8", pretty_print=True)
                return (tmpFile, True)

    return (None, False)

BatchQueryTemplate = """<?xml version="1.0"?>
<!-- WARNING: this file is automatically generated. Changes may be overwritten. -->
<ModelInterfaceBatch>
    <class name="ModelInterface.ModelGUI2.DbViewer">
        <command name="XMLDB Batch File">
            <scenario name="{scenario}"/>
            <queryFile>{queryFile}</queryFile>
            <outFile>{csvFile}</outFile>
            <xmldbLocation>{xmldb}</xmldbLocation>
            <batchQueryResultsInDifferentSheets>false</batchQueryResultsInDifferentSheets>
            <batchQueryIncludeCharts>false</batchQueryIncludeCharts>
            <batchQuerySplitRunsInDifferentSheets>false</batchQuerySplitRunsInDifferentSheets>
            <batchQueryReplaceResults>true</batchQueryReplaceResults>
        </command>
    </class>
</ModelInterfaceBatch>
"""

def runBatchQuery(scenario, queryName, queryPath, outputDir, xmldb=None,
                  csvFile=None, miLogFile=None, regions=GCAM_32_REGIONS,
                  regionMap=None, noRun=False, noDelete=False):
    """
    Run a query against GCAM's XML database given by `xmldb` (or computed
    from other parameters), optionally save the results into `outfile`,
    and return the data in a pandas `DataFrame`.

    :param scenario: (str) the name of the scenario to perform the query on
    :param queryName: (str) the name of a query to execute
    :param queryPath:
    :param outputDir:
    :param xmldb: (str) the pathname to the XML database to query
    :param csvFile: if None, query results are written to a computed filename.
    :param miLogFile:
    :param regions:
    :param regionMap:
    :param noRun:
    :param noDelete:
    :return:
    """
    basename = os.path.basename(queryName)
    mainPart, extension = os.path.splitext(basename)   # strip extension, if any

    # Look for both the literal name as given as well as the name with underscores replaced with spaces
    filename, isTempFile = _findOrCreateQueryFile(basename, queryPath, regions, regionMap=regionMap)

    if not filename:
        raise PygcamException("Error: file for query '%s' was not found." % basename)

    if not csvFile:
        csvFile = "%s-%s.csv" % (mainPart, scenario)    # compute default filename
        csvFile = csvFile.replace(' ', '_')             # eliminate spaces for convenience

    csvPath = os.path.abspath(os.path.join(outputDir, csvFile))

    # Create a batch file for ModelInterface to invoke the query on the named
    # scenario, and save the output where specified
    batchFile = getTempFile('.xml')
    batchFileText = BatchQueryTemplate.format(scenario=scenario, queryFile=filename, csvFile=csvPath, xmldb=xmldb)

    _logger.debug("Creating temporary batch file '%s'", batchFile)

    with open(batchFile, "w") as fp:
        fp.write(batchFileText)

    _logger.debug("Generating %s", csvPath)

    redirect = ">> %s 2>&1" % miLogFile if miLogFile else ''

    def copyToLogFile(logFile, filename, msg=''):
        with open(logFile, 'a') as m:
            with open(filename, 'r') as f:
                m.write(msg)
                map(m.write, f.readlines())

    if miLogFile:
        copyToLogFile(miLogFile, filename,  "Query file: '%s'\n\n" % filename)
        copyToLogFile(miLogFile, batchFile, "Batch file: '%s'\n\n" % batchFile)

    jarFile     = os.path.normpath(getParam('GCAM.JarFile'))
    javaArgs    = getParam('GCAM.JavaArgs')
    javaLibPath = getParam('GCAM.JavaLibPath')
    if javaLibPath:
        javaLibPath = os.path.normpath(javaLibPath) # do this separately to avoid turning "" into "."

    javaLibPathArg = '-Djava.library.path="%s"' % javaLibPath if javaLibPath else ""

    command = 'java %s %s -jar "%s" -b "%s" %s' % (javaArgs, javaLibPathArg, jarFile, batchFile, redirect)

    if noRun:
        print command
    else:
        _logger.debug(command)

    if not noRun:

        # TBD: this will be unnecessary with the next release of ModelInterface
        useVirtualBuffer = getParamAsBoolean('GCAM.UseVirtualBuffer')

        try:
            xvfb = None
            if useVirtualBuffer:
                xvfb = Xvfb()      # run the X virtual buffer to suppress popup windows

            subprocess.call(command, shell=True)

            # The java program always exits with 0 status, but when the query fails,
            # it writes an error message to the CSV file. If this occurs, we delete
            # the file.
            csvExists = False

            if os.path.exists(csvPath):
                csvExists = True
                with open(csvPath, 'r') as f:
                    line = f.readline()

            if not csvExists or line.find('java.land.Exception') >= 0:
                _logger.error("Query '%s' failed.", queryName)
                if csvExists:
                    _logger.debug("Deleting '%s'", csvPath)
                    os.remove(csvPath)

            # Deprecated -- fails on Windows
            # testCommand = "head -1 '%s' | grep -q 'java.lang.Exception:'" % csvPath
            # if subprocess.call(testCommand, shell=True) == 0:
            #     _logger.error("Query '%s' failed.\nDeleting '%s'", queryName, csvPath)
            #     os.remove(csvPath)

        except:
            raise

        finally:        # caller (runGCAM function) traps exceptions, so we just re-raise
            if noDelete:
                _logger.debug("Not deleting tmp batch file '%s'", batchFile)
            else:
                _logger.debug("Deleting tmp batch file '%s'", batchFile)
                os.remove(batchFile)

            if xvfb:
                xvfb.terminate()

            if isTempFile:
                if noDelete:
                    _logger.debug("Not deleting tmp file '%s'", filename)
                else:
                    _logger.debug("Deleting tmp file '%s'", filename)
                    os.remove(filename)

def ensureCSV(file):
    return ensureExtension(file, '.csv')

# def sumYears(files, skiprows=1, interpolate=False):
#     csvFiles = map(ensureCSV, files)
#     dframes  = map(lambda fname: readCsv(fname, skiprows=skiprows, interpolate=interpolate), csvFiles)
#
#     # TBD: preserve columns that have a single value only? Maybe this collapses into sumYearsByGroup()?
#     for df, fname in zip(dframes, csvFiles):
#         root, ext = os.path.splitext(fname)
#         outFile = root + '-sum' + ext
#         yearCols = filter(str.isdigit, df.columns)
#
#         with open(outFile, 'w') as f:
#             sums = df[yearCols].sum()
#             csvText = sums.to_csv(None)
#             f.write("%s\n%s\n" % (label, csvText))

# TBD: pass an output directory?
def sumYearsByGroup(groupCol, files, skiprows=1, interpolate=False):
    import numpy as np

    csvFiles = map(ensureCSV, files)
    dframes  = map(lambda fname: readCsv(fname, skiprows=skiprows, interpolate=interpolate), csvFiles)

    for df, fname in zip(dframes, csvFiles):
        units = df['Units'].unique()
        if len(units) != 1:
            raise Exception, "Can't sum results; rows have different units: %s" % units

        root, ext = os.path.splitext(fname)
        name = groupCol.replace(' ', '_')     # eliminate spaces for general convenience
        outFile = '%s-groupby-%s%s' % (root, name, ext)

        cols = [groupCol] + filter(str.isdigit, df.columns)
        grouped = df[cols].groupby(groupCol)
        df2 = grouped.aggregate(np.sum)
        df2['Units'] = units[0]         # add these units to all rows

        with open(outFile, 'w') as f:
            csvText = df2.to_csv(None)
            label = outFile
            f.write("%s\n%s\n" % (label, csvText))

def csv2xlsx(inFiles, outFile, skiprows=0, interpolate=False):
    import pandas as pd

    csvFiles = map(ensureCSV, inFiles)
    # TBD: catch exception on reading bad CSV file; save error and report at the end
    dframes  = map(lambda fname: readCsv(fname, skiprows=skiprows, interpolate=interpolate), csvFiles)

    sheetNum = 1
    outFile = ensureExtension(outFile, '.xlsx')
    with pd.ExcelWriter(outFile, engine='xlsxwriter') as writer:
        workbook = writer.book
        linkFmt = workbook.add_format({'font_color': 'blue', 'underline': True})

        # Create an index sheet
        indexSheet = workbook.add_worksheet('index')
        indexSheet.write_string(0, 1, 'Links to query results')
        maxlen = max(map(len, csvFiles))
        indexSheet.set_column('B:B', maxlen)
        for i, name in enumerate(csvFiles):
            row = i+1
            indexSheet.write(row, 0, row)
            indexSheet.write_url(row, 1, "internal:%d!A1" % row, linkFmt, name)
            #indexSheet.write(row, 1, '=hyperlink("#%d!A1", "%s")' % (row, name), linkFmt)

        for df, fname in zip(dframes, csvFiles):
            sheetName = str(sheetNum)
            sheetNum += 1
            dropExtraCols(df, inplace=True)
            df.to_excel(writer, index=None, sheet_name=sheetName, startrow=3, startcol=0)
            worksheet = writer.sheets[sheetName]
            worksheet.write_string(0, 0, "Filename:")
            worksheet.write_string(0, 1, fname)
            #  =HYPERLINK("#1!A1", "Joe")
            #worksheet.write(1, 0, '=hyperlink("#index!A1", "Back to index")', linkFmt)
            worksheet.write_url(1, 0, "internal:index!A1", linkFmt, "Back to index")


def writeDiffsToCSV(outFile, referenceFile, otherFiles, skiprows=1, interpolate=False, percentage=False):
    refDF = readCsv(referenceFile, skiprows=skiprows, interpolate=interpolate)

    with open(outFile, 'w') as f:
        for otherFile in otherFiles:
            otherFile = ensureCSV(otherFile)   # add csv extension if needed
            otherDF   = readCsv(otherFile, skiprows=skiprows, interpolate=interpolate)

            diff = computeDifference(refDF, otherDF, percentage=percentage)

            csvText = diff.to_csv(None)
            label = "[%s] minus [%s]" % (otherFile, referenceFile)
            if percentage:
                label = "(%s)/%s" % (label, referenceFile)
            f.write("%s\n%s" % (label, csvText))    # csvText has "\n" already


def writeDiffsToXLSX(outFile, referenceFile, otherFiles, skiprows=1,
                     interpolate=False, percentage=False):
    import pandas as pd

    with pd.ExcelWriter(outFile, engine='xlsxwriter') as writer:
        sheetNum = 1
        _logger.debug("Reading reference file:", referenceFile)
        refDF = readCsv(referenceFile, skiprows=skiprows, interpolate=interpolate)

        for otherFile in otherFiles:
            otherFile = ensureCSV(otherFile)   # add csv extension if needed
            _logger.debug("Reading other file:", otherFile)
            otherDF   = readCsv(otherFile, skiprows=skiprows, interpolate=interpolate)

            sheetName = 'Diff%d' % sheetNum
            sheetNum += 1

            diff = computeDifference(refDF, otherDF, percentage=percentage)

            diff.reset_index(inplace=True)      # convert multi-index into regular column values
            diff.to_excel(writer, index=None, sheet_name=sheetName, startrow=2, startcol=0)

            #workbook  = writer.book
            #worksheet = workbook.add_worksheet(sheetName)
            worksheet = writer.sheets[sheetName]
            label     = "[%s] minus [%s]" % (otherFile, referenceFile)
            if percentage:
                label = "(%s)/%s" % (label, referenceFile)
            worksheet.write_string(0, 0, label)

            startRow = diff.shape[0] + 4
            worksheet.write_string(startRow, 0, otherFile)
            startRow += 2
            otherDF.reset_index(inplace=True)
            otherDF.to_excel(writer, index=None, sheet_name=sheetName, startrow=startRow, startcol=0)

        dropExtraCols(refDF, inplace=True)
        _logger.debug("writing DF to excel file", outFile)
        refDF.to_excel(writer, index=None, sheet_name='Reference', startrow=0, startcol=0)


def writeDiffsToFile(outFile, referenceFile, otherFiles, ext='csv',
                     skiprows=1, interpolate=False, percentage=False):
    if ext == '.csv':
        writeDiffsToCSV(outFile, referenceFile, otherFiles, skiprows=skiprows,
                        interpolate=interpolate, percentage=percentage)
    else:
        writeDiffsToXLSX(outFile, referenceFile, otherFiles, skiprows=skiprows,
                         interpolate=interpolate, percentage=percentage)

# TBD: either use this internally or, if no advantage, drop it as unnecessary
class GcamResult(object):
#     """
#     Holds the result of a batch query against GCAM's XML database and provides access
#     to various methods to manipulate these results. (This class is a object wrapper
#     around the functions defined in this module.)
#
#     :param filename: the path to a .csv file in the usual GCAM format.
#     :interpolate: if ``True`` annual values are interpolated.
#     :param years: a sequence of two years (str or int); only in this range (inclusive)
#       are kept. Data for other years is dropped.
#     :param interpolate: if ``True``, annual values are linearly interpolated between
#       timesteps.
#     :param startYear: If non-zero, begin interpolation at this year, which
#       must be the name of a column in the `DataFrame`.
#     :param exitOnError: if ``False``, we trap the error and exit, otherwise we pass
#           it on.
#     """
#     def __init__(self, filename, years=None, interpolate=False, startYear=0):
#         self.filename = filename
#         self.df = self.readCsv(filename, years=years, interpolate=interpolate,
#                                startYear=startYear)
#
#     def limitYears(self, years):
#         """
#         Modify self.df to drop all years outside the range given by `years`.
#
#         :param years: a sequence of two years (str or int); only in this range (inclusive)
#           are kept. Data for other years is dropped.
#         :return: none; ``self.df`` is modified in place.
#         """
#         limitYears(self.df, years)
#
#     def interpolateYears(self, startYear=0, inplace=False):
#         """
#         Interpolate linearly between each pair of years in the GCAM output. The
#         timestep is calculated from the numerical (string) column headings given
#         in the `DataFrame`_ `df`, which are assumed to represent years in the time-series.
#         The years to interpolate between are read from `df`, so there's no dependency
#         on any particular time-step, or even on the time-step being constant.
#
#         :param df: a `DataFrame`_ holding data of the format returned by batch queries
#           on the GCAM XML database
#         :param startYear: If non-zero, begin interpolation at this year.
#         :param inplace: If True, modify `self.df` in place; otherwise modify a copy.
#         :return: if `inplace` is True, `self.df` is returned; otherwise a copy
#           of `self.df` with interpolated values is returned.
#         """
#         return interpolateYears(self.df, startYear=startYear, inplace=inplace)
#
#     def readCsv(self, filename, years=years,
#                 interpolate=interpolate, startYear=startYear):
#         """
#         Read a .csv file of the form generated by batch queries on the GCAM
#         XML database. Basically this is a generic .csv file with an extra
#         header row. Based on the function :py:func:`readCsv`.
#
#         :param filename: the .csv file to read
#         :param years: a sequence of two years (str or int); only in this range (inclusive)
#           are kept. Data for other years is dropped.
#         :param interpolate: if ``True``, annual values are linearly interpolated between
#           timesteps.
#         :param startYear: If non-zero, begin interpolation at this year, which
#           must be the name of a column in the `DataFrame`.
#         :return: a `DataFrame` containing the data from the .csv file, which is also
#           stored in `self.df`.
#         """
#         self.df = readCsv(filename, years=years, interpolate=interpolate, startYear=startYear)
#         return self.df
#
#     @classmethod
#     def computeDifference(cls, obj1, obj2):
#         """
#         Compute the difference between the `DataFrames`_ in two GcamResult instances.
#
#         :param obj1: a `GcamResult` instance
#         :param obj2: a `GcamResult` instance
#         :return: a `DataFrame`_ with the difference in all the year columns, computed
#           as (obj2.df - obj1.df).
#         """
#         diff = computeDifference(obj1.df, obj2.df)
#         return diff
#
#     def dropExtraCols(self, inplace=True):
#         """
#         Drop some columns that GCAM queries sometimes return, but which we
#         generally don't need. The columns dropped are ['scenario', 'Notes', 'Date'].
#         Based on the function :py:func:`dropExtraCols`.
#
#         :param inplace: if True, modify `df` in-place; otherwise return a modified copy.
#         :return: the original `df` (if inplace=True) or the modified copy.
#         """
#         df = dropExtraCols(self.df, inplace=inplace)
#         return df
    pass


def main(args):
    readConfigFiles(args.configSection)

    miLogFile   = getParam('GCAM.ModelInterfaceLogFile')
    outputDir   = args.outputDir or getParam('GCAM.OutputDir')
    workspace   = args.workspace or getParam('GCAM.RunWorkspaceRoot')
    xmldb       = args.xmldb     or os.path.join(workspace, 'output', getParam('GCAM.DbFile'))
    queryPath   = args.queryPath or getParam('GCAM.QueryPath')
    regionFile  = args.regionMap or getParam('GCAM.RegionMapFile')
    regions     = args.regions.split(',') if args.regions else GCAM_32_REGIONS
    scenarios   = args.scenario.split(',')
    queryNames  = args.queryName

    _logger.debug("Query names: '%s'", queryNames)

    if not queryNames:
        raise PygcamException("Error: At least one query name must be specified")

    mkdirs(outputDir)

    xmldb = os.path.abspath(xmldb)

    _logger.info('region map file: %s', regionFile)
    regionMap = readRegionMap(regionFile) if regionFile else None

    if miLogFile:
        miLogFile = os.path.abspath(miLogFile)
        if os.path.lexists(miLogFile):
            os.unlink(miLogFile)       # remove it, if any, to start fresh

    for scenario in scenarios:
        for queryName in queryNames:
            queryName = queryName.strip()

            if not queryName or queryName[0] == '#':    # allow blank lines and comments
                continue

            _logger.debug("Processing query '%s'", queryName)

            runBatchQuery(scenario, queryName, queryPath, outputDir, xmldb=xmldb,
                          miLogFile=miLogFile, regions=regions, regionMap=regionMap,
                          noRun=args.noRun, noDelete=args.noDelete)
